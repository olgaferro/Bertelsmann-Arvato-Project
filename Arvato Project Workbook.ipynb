#!/usr/bin/env python
# coding: utf-8

# # Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services
# 
# In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.
# 
# The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings.

# In[ ]:





# In[1]:


# import libraries here; add more as necessary
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import Imputer, StandardScaler
from sklearn.decomposition import PCA




import seaborn as sns
import ast

# magic word for producing visualizations in notebook
get_ipython().run_line_magic('matplotlib', 'inline')


# ## Part 0: Get to Know the Data
# 
# There are four data files associated with this project:
# 
# - `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).
# - `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).
# - `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).
# - `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).
# 
# Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers ("CUSTOMERS") are similar to or differ from the general population at large ("AZDIAS"), then use your analysis to make predictions on the other two files ("MAILOUT"), predicting which recipients are most likely to become a customer for the mail-order company.
# 
# The "CUSTOMERS" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original "MAILOUT" file included one additional column, "RESPONSE", which indicated whether or not each recipient became a customer of the company. For the "TRAIN" subset, this column has been retained, but in the "TEST" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.
# 
# Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.
# 
# In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.
# 
# You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them.

# In[2]:


# load in the data
azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';')
customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')


# # Part 0: Look at the Data
# 

# ### a.1)- Customers DataFrame:  Demographics data for customers of a mail-order company

# In[3]:


customers.head(10)


# In[4]:


customers.describe()


# In[5]:


customers.info()


# ### - Analizing missing data in customers dataframe

# In[6]:


# Number of no missing columns
n_no_missing_columns = len(customers.columns[customers.isnull().mean()==0])
print(n_no_missing_columns)


# In[7]:


# Sum nulls values in each row
sum(customers.isnull().sum(axis=1))


# In[8]:


# In the first look in the data we can see there is a lot of null data


# In[9]:


### output percentage of values that are missing by columns
missing_cols = 100 * customers.isnull().sum() / customers.shape[0]


# In[10]:


plt.hist(missing_cols, bins=40);

plt.xlabel('Missing values')
plt.ylabel('Customers features')
plt.title('Proportion of missing values in Customers features')


# In[11]:



#Most of the columns have percentage of missing values of less than 30%. 


# In[12]:


## Function to detect nulls in columns

def find_columns_nulls(df,percent) :
    
    '''
    Find the columns with null bigger than 'percent'
    
    :df -> it´s the dataframe to explore for null in columns
    :percent-> its the % of columns with null values
   
    :return ->list_columns_nulls: its a list with more than  percent of null values
    '''
    
    list_columns_nulls = []

    for col in df.columns:

           if  ((df[col].isnull().sum()/df.shape[0])>percent):
     
            list_columns_nulls.append(col)       
            
        
    return list_columns_nulls


# In[13]:



list_customers_columns_nulls = find_columns_nulls(customers,0.30)

print(list_customers_columns_nulls)


# In[14]:


## Function to show columns with nulls

def display_null_columns(df,name,list_nulls,percent) :
    
    '''
    Show the plot where the columns of dataframe (dt) has more than 'percent' of nulls values
    
    :df -> it´s the dataframe to explore for null in columns
    :list_nulls -> list with nulls columns
    
    :return ->show the plot
    '''
    nombre = str(df)
    data=[]
    #data = [i for i in (df.isna().sum()/df.shape[0]) if i >=percent]
    
    for i in (df.isna().sum()/df.shape[0]):
          if (i >=percent):
                print(i)
                data.append(i)
                

    y_pos = np.arange(len(list_nulls))

    
    f, ax = plt.subplots(figsize=(5,8))
    plt.barh(y_pos, data, align='center', alpha=percent,color = sns.color_palette("husl", 9))
    plt.yticks(y_pos, list_nulls)
    plt.xlabel('%')
    plt.title('Percentage of columns in DataFrame: {}'.format(name))
    
  

   
    return  plt.show()
            
  


# In[15]:


display_null_columns(customers,'customers',list_customers_columns_nulls,0.30)


# ### - Dropping columns

# #List of columns with missing values greater than % are:
# 
# 
# ''''
# ALTER_KIND1 - No information
# 
# ALTER_KIND2 - No information
# 
# ALTER_KIND3 - No information
# 
# ALTER_KIND4 - No information
# 
# 
# 
# KK_KUNDENTYP - consumption movement in the last 12 months
# '''

# In[16]:


customers.drop(list_customers_columns_nulls, axis=1,inplace=True)


# In[17]:


#columns with categorical values
customers.select_dtypes('object').head()


# ### a.2) General AZDIAS DataFrame: Demographics data for the general population of Germany

# In[18]:


### a)- Customers DataFrame:


# In[19]:


azdias.head()


# In[20]:



azdias.describe()


# ### - Analizing missing data in AZDIAS dataframe

# In[21]:


# Number of no missing columns
n_no_missing_columns = len(azdias.columns[azdias.isnull().mean()==0])
print(n_no_missing_columns)


# In[22]:


###DROPING COLUMNS


# In[23]:


### output percentage of values that are missing by columns
missing_cols=100 * azdias.isnull().sum() / azdias.shape[0]


# In[24]:


plt.hist(missing_cols, bins=40);

plt.xlabel('Missing values')
plt.ylabel('Customers features')
plt.title('Proportion of missing values in Customers features')


# In[25]:


# Sum nulls values in each row
sum(azdias.isnull().sum(axis=1))


# In[26]:


# List of columns with a more than  % nulla
list_azdias_columns_nulls = find_columns_nulls(azdias,0.20)


print(list_azdias_columns_nulls)


# #List of columns with missing values greater than % are:
# 
# '''
# 
# ALTER_KIND1 - No information
# 
# ALTER_KIND2 - No information
# 
# ALTER_KIND3 - No information
# 
# ALTER_KIND4 - No information
# 
# 
# KK_KUNDENTYP - consumption movement in the last 12 months
# '''

# In[27]:


# showing the number of columns with % nulls
display_null_columns(azdias,'azdias',list_azdias_columns_nulls,0.20)


# ### - Dropping columns

# In[28]:


azdias.drop(list_azdias_columns_nulls,axis=1,inplace=True)


# In[29]:


# checking AZDIAS dataset shape
#azdias.shape


# In[ ]:


### output percentage of values that are missing by columns
missing_cols=100 * azdias.isnull().sum() / azdias.shape[0]


# In[ ]:


plt.hist(missing_cols, bins=40);

plt.xlabel('Missing values')
plt.ylabel('Customers features')
plt.title('Proportion of missing values in Customers features')


# In[ ]:


azdias.shape


# In[ ]:


#columns with categorical values
azdias.select_dtypes('object').head()


# In[59]:


customers.shape


# In[ ]:





# In[ ]:





# ### b.1) Loading Attributes files

# In[ ]:


dias_attributes_val = pd.read_excel('./DIAS Attributes - Values 2017.xlsx', header=1)
dias_attributes_levels = pd.read_excel('./DIAS Information Levels - Attributes 2017.xlsx', header=1)


# In[ ]:


#dias_attributes_val


# In[ ]:


#dias_attributes_levels


# ### b.2) Discard columns with nulls in atributes file 
# 

# In[ ]:


dias_attributes_val_new = dias_attributes_val[dias_attributes_val.index.isin(list_azdias_columns_nulls)==False]
dias_attributes_val_new.head()


# In[ ]:


dias_attributes_levels_new = dias_attributes_levels[dias_attributes_levels.index.isin(list_azdias_columns_nulls)==False]
dias_attributes_levels_new.head()


# ### b.3a) Refill the "Information level" column with ffil in attibutes info file

# In[ ]:


dias_attributes_levels_new.reset_index(drop=True, inplace=True)
dias_attributes_levels_new['Information level'].ffill(inplace=True)
dias_attributes_levels_new

dias_attributes_levels_new['Information level'].isnull().sum()


# In[ ]:


#we have still one null in Information level, we are going to apply bfill


# In[ ]:


dias_attributes_levels_new['Information level'].bfill(inplace=True)
dias_attributes_levels_new.head(25)


# ### b.4) Refill the "Information level" column with ffil in attributes info file

# In[ ]:


# now all the column Information level is complete
#dias_attributes_levels_new['Information level'].isnull().sum()


# In[ ]:


### b.5) Filling the blanks 
#dias_attributes_levels_new.ffill(inplace=True)


# In[ ]:


#dias_attributes_levels_new.head(25)
#removing Additional notes
dias_attributes_levels_new.drop('Additional notes',axis=1,inplace=True)


# In[ ]:


### b.3a) Refill the  column with ffil in attibutes  values


# In[ ]:


dias_attributes_val_new.reset_index(drop=True, inplace=True)
dias_attributes_val_new.ffill(inplace=True)
dias_attributes_val_new


# In[ ]:


### b.4) to list all the unknows values


# In[ ]:


dias_attributes_val_unknowns = dias_attributes_val_new.loc[dias_attributes_val_new.Meaning=='unknown', :][['Attribute', 'Value']]
dias_attributes_val_unknowns.head(20)


#dias_attributes_val_unknowns= pd.Series(dias_attributes_val_new.Meaning=='unknown',dias_attributes_val_new['Value'].values, index=dias_attributes_val_new['Attribute'])


# In[ ]:


dias_attributes_val_unknowns.reset_index(drop=True, inplace=True)
#dias_attributes_unknowns.ffill(inplace=True)
dias_attributes_val_unknowns.head(20)


# In[ ]:


#we only keep the rows whose have more than 50% not nulls

#azdias = azdias[azdias.isnull().sum(axis=1)<=180]
#customers = customers[customers.isnull().sum(axis=1)<=183]


# In[ ]:


### b.5) unknows. We can see there are differents kinds of unknows, for example -1,0 and both


# In[ ]:



def replace_unknowns(df, attributes_unknowns):
    '''
      Replace unkowns values for nan from dataframe df 
      
      :df -> Dataframe to change the value of attributes with unknows values
      :attributes_unknowns -> list of attributes whose are unknow 
      
      return
      
    '''
    for column in df.columns:
        if df[column].dtype == np.int64:
            df[column] = df[column].astype(np.float64)
        if column not in set(attributes_unknowns['Attribute']):
            continue
        
        unknown_value = attributes_unknowns.loc[attributes_unknowns['Attribute'] == column].Value.item()
        unknown_value = set(float(x) for x in unknown_value.split(', ')) if type(unknown_value) is str else set([float(unknown_value)])
        df[column] = df[column].mask(df[column].isin(unknown_value), other=np.nan)
    

    
    return df


# In[ ]:


#levels in attributes
#print(list(dias_attributes_val_unknowns['Information level'].unique()))


# In[ ]:


customers.head()


# In[ ]:



customer = replace_unknowns(customers, dias_attributes_val_unknowns)
customer.head(25)


# In[ ]:


azdias = azdias.iloc[:100000]

azdias = replace_unknowns(azdias, dias_attributes_val_unknowns)
azdias.head(25)


# In[ ]:



def encode_col_PRAEGENDE(val):
    
    '''
    Re-encoding for the column -PRAEGENDE_JUGENDJAHRE-
    
    :val-> it´s the value to encode
    
    :return -> 0: Mainstream, 1: Avangarde
    '''
    mainstream = [1, 3, 5, 8, 10, 12, 14]
    avantgarde = [2, 4, 6, 7, 9, 11, 13, 15]
    
    if val in mainstream: 
        return 0
    elif val in avantgarde: 
        return 1
    else:
        return val


# In[ ]:




def encode_cols_LP_(df):
    
    '''
    Re-encoding for  all the columns -LP_*-
    replacing zero with NaN values, dropping duplicate columns
    
    
    :df-> dataframe to treat LP_* columns
    
    :return df with encoding columns
    
    '''
    # droping complex and/or duplicate columns
    df.drop(['LP_FAMILIE_FEIN','LP_STATUS_FEIN','LP_LEBENSPHASE_FEIN','LP_LEBENSPHASE_GROB'], axis=1, inplace=True)
  
    # replacing 0s with NaN values
    cols = ["LP_LEBENSPHASE_FEIN","LP_LEBENSPHASE_GROB", "LP_FAMILIE_FEIN", "LP_FAMILIE_GROB"]
    df[cols] = df[cols].replace({0: np.nan})
    
    
    # re-encode LP_STATUS_GROB and LP_FAMILIE_GROB
    status = {1: 1, 2: 1, 3: 2, 4: 2, 5: 2, 6: 3,  7: 3, 8: 4, 9: 4, 10: 5}
    familie = {1: 1, 2: 2, 3: 3, 4: 3, 5: 3, 6: 4, 7: 4, 8: 4, 9: 5, 10: 5, 11: 5}
    
    df["LP_STATUS_GROB"] = df["LP_STATUS_GROB"].map(status)                                              
    df["LP_FAMILIE_GROB"] = df["LP_FAMILIE_GROB"].map(familie)    
    
    return df


# In[ ]:


def clean_data(df, attributes_values, nan_threshold):
     
    '''
    Cleaning data, an preprocessing in dataframe df
    
    :df-> DataFrame to treat data to clean up
    :attributes_values-> DataFrame attributes to located unknows values
    :nan_threshold-> limit for dropping rows with more than % NaN values
        
        
    :return df_cl-> df cleaned and preprocessed
    '''
    print("1")
    #replacing values X, XX with NaN values 
    #df.replace('X',np.nan,inplace=True)
    #df.replace('XX',np.nan,inplace=True)
    print("2")
    
    # dropping rows with %missing values highter than nan_threshold
    percent_missing_row = df.isnull().mean(axis=1) * 100
    df = df[percent_missing_row <= nan_threshold]

    # Reencoding CATEGORICAL & MIXED FEATURES. Processing datetime
    #df["EINGEFUEGT_AM"] = pd.to_datetime(df["EINGEFUEGT_AM"])
    # df["EINGEFUEGT_AM"] = df['EINGEFUEGT_AM'].map(lambda x: x.year)
    print("3")
    # treat 'X' and 'XX' as nan values
    df['CAMEO_DEUG_2015'] = df['CAMEO_DEUG_2015'].replace('X', np.nan)
    df['CAMEO_INTL_2015'] = df['CAMEO_INTL_2015'].replace('XX', np.nan)
    
    
    df['CAMEO_DEUG_2015'] = df['CAMEO_DEUG_2015'].astype(float)
    df['CAMEO_INTL_2015'] = df['CAMEO_INTL_2015'].astype(float)
    
    print("4")
    # Encoding binaries values
    df['OST_WEST_KZ'] = df['OST_WEST_KZ'].map({'W': 0, 'O': 1})
    
    # dropping categorical variables
    df.drop(['CAMEO_DEU_2015', 'EINGEFUEGT_AM'], axis=1, inplace=True)
    
    print("5")
    # re encoding CAMEO_INTL_2015
    df['CAMEO_INTL_2015_WEALTH_LEVEL'] = df['CAMEO_INTL_2015'].apply(lambda x: np.floor(pd.to_numeric(x)/10))
    df['CAMEO_INTL_2015_STATUS'] = df['CAMEO_INTL_2015'].apply(lambda x: pd.to_numeric(x)%10)
    df.drop('CAMEO_INTL_2015', axis=1, inplace=True)
    
    # Re encoding columns LP*
    #df = encode_cols_LP_(df)
    print('6')
    # re-encode PRAEGENDE_JUGENDJAHRE
    #df['PRAEGENDE_JUGENDJAHRE'] = df['PRAEGENDE_JUGENDJAHRE'].apply(lambda x: encode_col_PRAEGENDE(x))
    
    print('7')
    # impute missing values
    #imputer = SimpleImputer(missing_values=np.nan, strategy='median')
    #imputer = Imputer(missing_values=np.nan, strategy='mean', axis=0)
    #df_clean = pd.DataFrame(imputer.fit_transform(df))
    
    return df


# In[ ]:


#customers = customers.iloc[:10000]
#azdias = azdias.iloc[:10000]


# In[ ]:


customers_cleaned = clean_data(customers,dias_attributes_val_new, 30)


# In[ ]:


customers_cleaned.shape


# In[ ]:



azdias_cleaned = clean_data(azdias,dias_attributes_val_new, 30)


# In[ ]:


azdias_cleaned.shape


# In[ ]:


### b.7)  impute missing values


# In[ ]:


# impute missing values

imputer = Imputer(missing_values=np.nan, strategy='mean') 


# In[ ]:


customers_cleaned[customers_cleaned.columns] = imputer.fit_transform(customers_cleaned)


# In[ ]:


#customers_imputed.shape
customers_cleaned.shape


# In[ ]:


azdias_cleaned[azdias_cleaned.columns] = imputer.fit_transform(azdias_cleaned)


# In[ ]:


#azdias_imputed.shape


# In[ ]:


### b.7)  Data scaled


# In[ ]:


## Using Data scaling  we can is a pre-processed step. 
## This estimator scales and translates each feature individually so that it falls in the given range in the training set.

# Before use PCA we need to scale features to be of the same range.  PCA algorithm needs to have same scales in the features may be influenced by variations 


# In[ ]:



# Apply feature scaling to the general population demographics data(azdias).
scaler = StandardScaler()
#scaler = MinMaxScaler()

azdias_cleaned[azdias_cleaned.columns] = scaler.fit_transform(azdias_cleaned)
#customers_scaled = scaler.fit(customers_cleaned)


# In[ ]:


#customers_scaled = pd.read_pickle('customers_scaled.pkl')
#customers_scaled.head()


# In[ ]:


#azdias_scaled = pd.read_pickle('azdias_scaled.pkl')
#azdias_scaled.head()


# In[ ]:


print('Covariance matrix: \n%s' %np.cov(azdias_cleaned.T))


# In[ ]:


### b.5) exclusive columns in customer and azdias 


# In[ ]:


def get_exclusive_cols(df):
    attributes_without_info = []
    for col in df.columns:
        if col not in attributes_values.Attribute.values:
            attributes_without_info.append(col)
    return attributes_without_info


# In[ ]:





# ## Part 1: Customer Segmentation Report
# 
# The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so.

# In[ ]:


### c) Appling sklearn PCA class


# In[ ]:


# using PCA (Principal Component Analysis PCA) analysis we can reduce of dimensionality of our data without loss information.
# we can reduce complexity 


# In[ ]:


# Apply PCA to the data.

%%time
pca = PCA()


# In[ ]:


#pca_azdias = pca.fit(azdias_scaled)
pca_azdias = pca.fit_transform(cleaned_azdias)


# In[ ]:


def show_plot(pca): 
    '''
    Creates a plot associated with the principal components 
    
    :pca -> the result of instantian of PCA 
            
    Return -> graphic with the componenets
    
    '''
    plt.plot(np.cumsum(pca.explained_variance_ratio_))
    plt.title('Graphic:  Principal Components -  AZDIAS')
    plt.xlabel('Number of Components')
    plt.ylabel('Variance')

plt.show()


# In[ ]:


show_plot(pca_azdias)


# In[ ]:


#customers pca


# In[ ]:



print('NumPy covariance matrix: \n%s' %np.cov(customers_scaled.T))


# In[ ]:


pca_customers = pca.fit_transform(customers_scaled)


# In[ ]:


show_plot(pca_customers)


# #PCA class will be used to apply principal component analysis on the data, thus finding the vectors of maximal variance in the data. To start, at least half the number of features will be set (so there's enough features to see the general trend in variability).
# The ratio of variance explained by each principal component as well as the cumulative variance explained will be checked by plotting the cumulative or sequential values using matplotlib's plot() function. Based on the findings, a value for the number of transformed features will be retained for the clustering part of the project.
# Once a choice for the number of components to keep has been made, the PCA instance will be re-fit to perform the decided-on transformation.
# 

# ## Part 2: Supervised Learning Model
# 
# Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the "MAILOUT" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.
# 
# The "MAILOUT" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the "TRAIN" partition, which includes a column, "RESPONSE", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the "TEST" partition, where the "RESPONSE" column has been withheld.

# In[ ]:


mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')
mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';')


# In[ ]:


## Part 2: a)Get to Know the Data


# In[ ]:


mailout_train.shape


# In[ ]:


sns.countplot('RESPONSE', data=mailout_train)


# In[ ]:


# Let's check how the data is balanced
(mailout_train.RESPONSE == 1).mean()


# In[ ]:


## Checking columns with nulls
list_mailout_train_nulls = find_columns_nulls(mailout_train,0.30)

print(list_mailout_train_nulls)


# In[ ]:


list_mailout_test_nulls = find_columns_nulls(mailout_test,0.30)

print(list_mailout_test_nulls)


# The responser only are th 1,24%. 
# 
# ROC AUC (Receiver Operating Characteristic Area Under the Curve) is a metric used to evaluate the quality and performance of a binary classification model.
# 
# In a binary classification problem, where the goal is to predict between two classes (e.g., "positive" and "negative"), the model assigns a score or probability to each example indicating the confidence in the classification. The ROC curve is constructed by plotting the true positive rate (sensitivity) against the false positive rate (1-specificity) at different classification thresholds.
# 
# ##the metric to use, as obvsiouly accuracy won't give us the information we need. We will use ROC AUC instead
# 
# 
# the cross validation strategy: we will use Stratified KFold, a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.

# In[ ]:


## Part 2: b)Cleanning data


# In[ ]:


# Cleaning data
mailout_clean = clean_data(mailout_train,dias_attributes_val_new,30)


# In[ ]:


# Missing values
mailout_clean.isnull().sum().sum()


# In[ ]:


# Droping LNR, this col doesnt have 
mailout_clean.drop('LNR', axis=1, inplace=True)


# In[ ]:


# split data to features and labels
target = mailout_clean['RESPONSE']

# dropping RESPONSE variable
features = mailout_clean.drop('RESPONSE', axis=1)


# In[ ]:


# Imputer
imputed_mailout = pd.DataFrame(imputer.fit_transform(features.values), columns=features.columns)

# feature scaling
scaled_mailout = pd.DataFrame(scaler.fit_transform(imputed_mailout.values), columns=imputed_mailout.columns)


# In[ ]:




